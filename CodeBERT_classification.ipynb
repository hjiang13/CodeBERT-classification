{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNn1ztSlCWoiTcIM/V8qXhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjiang13/CodeBERT-classification/blob/main/CodeBERT_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IhUWrknKglL",
        "outputId": "6a3fb7a2-502a-43f7-d5a7-9590f0c7b44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeBERT-classification'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 87 (delta 44), reused 28 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (87/87), 3.25 MiB | 6.31 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hjiang13/CodeBERT-classification.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfMwgN7lMeOq",
        "outputId": "38d1fbc9-7e96-4d56-d6cf-90fc4644ccc4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r CodeBERT-classification/"
      ],
      "metadata": {
        "id": "lP66LXChVhrp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('CodeBERT-classification/')"
      ],
      "metadata": {
        "id": "kd7sp0TtMgKO"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "V_f8RZduFnpN",
        "outputId": "ba85f6cc-cd29-4227-afac-5caf4d07ed50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating 88521ab..7294e2f\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tcode/train_resilience.sh\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('code')"
      ],
      "metadata": {
        "id": "JnFeRmTGMoi7"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../../')"
      ],
      "metadata": {
        "id": "dkT-XTG7FiJP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod u+x ./train.sh\n",
        "!./train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usDvPdyPMqTk",
        "outputId": "47465c03-19b0-4578-f3f3-c38925b41635"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "03/21/2024 03:38:56 - WARNING - __main__ -   device: cuda, n_gpu: 1\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/21/2024 03:38:57 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/test.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_name_or_path='neulab/codebert-cpp', tokenizer_name='neulab/codebert-cpp', block_size=256, do_train=True, do_eval=False, do_test=False, train_batch_size=8, eval_batch_size=16, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, warmup_steps=0, seed=123456, num_train_epochs=5, n_gpu=1, device=device(type='cuda'))\n",
            "03/21/2024 03:38:57 - INFO - __main__ -   Hailong: do training \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "03/21/2024 03:39:04 - INFO - __main__ -   ***** Running training *****\n",
            "03/21/2024 03:39:04 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 03:39:04 - INFO - __main__ -     Num Epochs = 5\n",
            "03/21/2024 03:39:04 - INFO - __main__ -     batch size = 8\n",
            "03/21/2024 03:39:04 - INFO - __main__ -     Total optimization steps = 3750\n",
            "  0%|          | 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "epoch 0 loss 0.94: 100%|██████████| 750/750 [01:34<00:00,  7.91it/s]\n",
            "03/21/2024 03:40:44 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 03:40:44 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 03:40:44 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 03:41:10 - INFO - __main__ -     eval_loss = 0.0055\n",
            "03/21/2024 03:41:10 - INFO - __main__ -     eval_acc = 0.9985\n",
            "03/21/2024 03:41:10 - INFO - __main__ -     ********************\n",
            "03/21/2024 03:41:10 - INFO - __main__ -     Best acc:0.9985\n",
            "03/21/2024 03:41:10 - INFO - __main__ -     ********************\n",
            "03/21/2024 03:41:16 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
            "epoch 1 loss 0.004: 100%|██████████| 750/750 [01:33<00:00,  8.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod u+x ./train_resilience.sh\n",
        "!./train_resilience.sh"
      ],
      "metadata": {
        "id": "K9xB9Kl4O4km",
        "outputId": "a03cbb19-02b1-45a8-d9a0-a99dc47bca6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "03/21/2024 04:48:13 - WARNING - __main__ -   device: cuda, n_gpu: 1\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/21/2024 04:48:14 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/test_resilience.json', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_name_or_path='neulab/codebert-cpp', tokenizer_name='neulab/codebert-cpp', block_size=256, do_train=True, do_eval=False, do_test=False, train_batch_size=8, eval_batch_size=16, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, warmup_steps=0, seed=123456, num_train_epochs=20, n_gpu=1, device=device(type='cuda'))\n",
            "03/21/2024 04:48:14 - INFO - __main__ -   Hailong: do training\n",
            "03/21/2024 04:48:14 - INFO - __main__ -   Hailong: train_dataset has been set\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "03/21/2024 04:48:15 - INFO - __main__ -   ***** Running training *****\n",
            "03/21/2024 04:48:15 - INFO - __main__ -     Num examples = 96\n",
            "03/21/2024 04:48:15 - INFO - __main__ -     Num Epochs = 20\n",
            "03/21/2024 04:48:15 - INFO - __main__ -     batch size = 8\n",
            "03/21/2024 04:48:15 - INFO - __main__ -     Total optimization steps = 240\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "epoch 0 loss 4.341: 100%|██████████| 12/12 [00:02<00:00,  4.99it/s]\n",
            "03/21/2024 04:48:23 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:48:23 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:48:23 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:48:49 - INFO - __main__ -     eval_loss = 4.5226\n",
            "03/21/2024 04:48:49 - INFO - __main__ -     eval_acc = 0.1487\n",
            "03/21/2024 04:48:49 - INFO - __main__ -     ********************\n",
            "03/21/2024 04:48:49 - INFO - __main__ -     Best acc:0.1487\n",
            "03/21/2024 04:48:49 - INFO - __main__ -     ********************\n",
            "03/21/2024 04:48:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
            "epoch 1 loss 3.471: 100%|██████████| 12/12 [00:01<00:00,  7.27it/s]\n",
            "03/21/2024 04:48:56 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:48:56 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:48:56 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:49:22 - INFO - __main__ -     eval_loss = 4.2065\n",
            "03/21/2024 04:49:22 - INFO - __main__ -     eval_acc = 0.1623\n",
            "03/21/2024 04:49:22 - INFO - __main__ -     ********************\n",
            "03/21/2024 04:49:22 - INFO - __main__ -     Best acc:0.1623\n",
            "03/21/2024 04:49:22 - INFO - __main__ -     ********************\n",
            "03/21/2024 04:49:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
            "epoch 2 loss 2.354: 100%|██████████| 12/12 [00:01<00:00,  7.20it/s]\n",
            "03/21/2024 04:49:30 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:49:30 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:49:30 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:49:56 - INFO - __main__ -     eval_loss = 3.8315\n",
            "03/21/2024 04:49:56 - INFO - __main__ -     eval_acc = 0.1623\n",
            "epoch 3 loss 1.719: 100%|██████████| 12/12 [00:01<00:00,  7.36it/s]\n",
            "03/21/2024 04:50:01 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:50:01 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:50:01 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:50:27 - INFO - __main__ -     eval_loss = 3.6357\n",
            "03/21/2024 04:50:27 - INFO - __main__ -     eval_acc = 0.1623\n",
            "epoch 4 loss 1.442: 100%|██████████| 12/12 [00:01<00:00,  6.81it/s]\n",
            "03/21/2024 04:50:33 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:50:33 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:50:33 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:50:59 - INFO - __main__ -     eval_loss = 3.5496\n",
            "03/21/2024 04:50:59 - INFO - __main__ -     eval_acc = 0.1623\n",
            "epoch 5 loss 1.272: 100%|██████████| 12/12 [00:01<00:00,  7.31it/s]\n",
            "03/21/2024 04:51:04 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:51:04 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:51:04 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:51:30 - INFO - __main__ -     eval_loss = 3.5021\n",
            "03/21/2024 04:51:30 - INFO - __main__ -     eval_acc = 0.1623\n",
            "epoch 6 loss 1.175: 100%|██████████| 12/12 [00:01<00:00,  6.62it/s]\n",
            "03/21/2024 04:51:36 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/21/2024 04:51:36 - INFO - __main__ -     Num examples = 6000\n",
            "03/21/2024 04:51:36 - INFO - __main__ -     Batch size = 16\n",
            "03/21/2024 04:52:01 - INFO - __main__ -     eval_loss = 3.5082\n",
            "03/21/2024 04:52:01 - INFO - __main__ -     eval_acc = 0.1623\n",
            "epoch 7 loss 1.077: 100%|██████████| 12/12 [00:01<00:00,  7.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8c5mUJukAcUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}